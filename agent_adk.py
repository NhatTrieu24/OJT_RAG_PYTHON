import os
import time
import psycopg2
import vertexai
from vertexai.language_models import TextEmbeddingModel
from tenacity import retry, stop_after_attempt, wait_exponential
import io
import requests
import pdfplumber
import re
import docx  # ThÆ° viá»‡n Ä‘á»c file Word (.docx)

# ==================== 1. Cáº¤U HÃŒNH AUTHENTICATION ====================
render_secret = "/etc/secrets/GCP_SERVICE_ACCOUNT_JSON"
local_key = "rag-service-account.json" 

if os.path.exists(render_secret): 
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = render_secret
    print("ğŸ”‘ [Auth] Sá»­ dá»¥ng Key tá»« Render Secrets.")
elif os.path.exists(local_key): 
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath(local_key)
    print("ğŸ”‘ [Auth] Sá»­ dá»¥ng Key tá»« file Local.")
else:
    print("âŒ [Auth] KhÃ´ng tÃ¬m tháº¥y Service Account Key!")

PROJECT_ID = os.getenv("PROJECT_ID", "reflecting-surf-477600-p4")
LOCATION = os.getenv("LOCATION", "us-west1")
DB_DSN = os.getenv("DB_DSN", "postgresql://postgres:123@caboose.proxy.rlwy.net:54173/railway")

embedding_model = None
try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    print("âœ… [Agent] Vertex AI & Embedding Model Ready.")
except Exception as e:
    print(f"âš ï¸ [Agent] Init Error: {e}")

# ==================== 2. HÃ€M Bá»” TRá»¢ (DRIVE & EMBEDDING) ====================

def get_text_from_drive(file_url):
    if not file_url or "drive.google.com" not in file_url: return ""
    try:
        # TÃ¡ch ID file
        file_id = re.search(r'[-\w]{25,}', file_url).group()
        # Sá»­ dá»¥ng link download trá»±c tiáº¿p cá»§a Google
        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
        
        response = requests.get(download_url, timeout=20, allow_redirects=True)
        if response.status_code == 200:
            stream = io.BytesIO(response.content)
            # Thá»­ Ä‘á»c PDF
            try:
                with pdfplumber.open(stream) as pdf:
                    return " ".join([p.extract_text() for p in pdf.pages[:5] if p.extract_text()])
            except:
                # Náº¿u khÃ´ng pháº£i PDF, thá»­ Ä‘á»c Word
                stream.seek(0)
                doc = docx.Document(stream)
                return " ".join([p.text for p in doc.paragraphs])
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c link: {e}")
    return ""



@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=5, min=10, max=120))
def get_embeddings_batch(texts):
    if not embedding_model or not texts: return []
    # LÃ m sáº¡ch text Ä‘á»ƒ tá»‘i Æ°u TPM (Tokens Per Minute)
    clean_texts = [str(t).replace("\n", " ").strip()[:2500] for t in texts if t]
    if not clean_texts: return []
    try:
        embeddings = embedding_model.get_embeddings(clean_texts)
        return [e.values for e in embeddings]
    except Exception as e:
        print(f"âš ï¸ API Warning: {e}. Äang Ä‘á»£i há»“i Quota...")
        raise e

def get_query_embedding(text):
    res = get_embeddings_batch([text])
    return res[0] if res else None

# ==================== 3. Äá»’NG Bá»˜ VECTOR THÃ”NG MINH (BATCH MODE) ====================

def sync_all_data(force_reset=False):
    print(f"ğŸ”„ [System] Báº¯t Ä‘áº§u Ä‘á»“ng bá»™ thÃ´ng minh (BATCH MODE - PDF & Word)...")
    conn = None
    try:
        conn = psycopg2.connect(dsn=DB_DSN)
        cur = conn.cursor()

        if force_reset:
            print("âš ï¸ [Reset] Äang Reset toÃ n bá»™ bá»™ nhá»› Vector...")
            tables = ["job_position", "company", "semester", "User", "major", "ojtdocument"]
            for t in tables:
                cur.execute(f'UPDATE "{t}" SET embedding = NULL, last_content_indexed = NULL;')
            conn.commit()

        scenarios = [
            {
                "table": "job_position",
                "id_col": "job_position_id",
                "sql": """
                    SELECT jp.job_position_id, 
                           'Vá»Š TRÃ: ' || COALESCE(jp.job_title, '') || '. CÃ”NG TY: ' || COALESCE(c.name, 'N/A') || 
                           '. YÃŠU Cáº¦U: ' || COALESCE(jp.requirements, 'KhÃ´ng cÃ³') as text
                    FROM job_position jp
                    LEFT JOIN semester_company sc ON jp.semester_company_id = sc.semester_company_id
                    LEFT JOIN company c ON sc.company_id = c.company_id
                """
            },
            {
                "table": "ojtdocument",
                "id_col": "ojtdocument_id",
                "sql": "SELECT ojtdocument_id, title, file_url FROM ojtdocument"
            },
            {
                "table": "semester",
                "id_col": "semester_id",
                "sql": "SELECT semester_id, 'Lá»ŠCH Ká»² Há»ŒC: ' || COALESCE(name, '') as text FROM semester"
            },
            {
                "table": "User",
                "id_col": "user_id",
                "sql": "SELECT user_id, 'Há»’ SÆ : ' || COALESCE(fullname, '') as text FROM \"User\""
            },
            {
                "table": "company",
                "id_col": "company_id",
                "sql": "SELECT company_id, 'CÃ”NG TY: ' || COALESCE(name, '') as text FROM company"
            },
            {
                "table": "major",
                "id_col": "major_id",
                "sql": "SELECT major_id, 'NGÃ€NH Há»ŒC: ' || COALESCE(major_title, '') as text FROM major"
            }
        ]

        for sc in scenarios:
            table = sc['table']
            cur.execute(f"""
                WITH latest AS ({sc['sql']})
                SELECT l.* FROM latest l
                LEFT JOIN "{table}" t ON l.{sc['id_col']} = t."{sc['id_col']}"
                WHERE t.embedding IS NULL OR t.last_content_indexed IS NULL;
            """)
            rows = cur.fetchall()

            if rows:
                print(f"ğŸ“¦ Báº£ng [{table}]: PhÃ¡t hiá»‡n {len(rows)} dÃ²ng cáº§n xá»­ lÃ½.")
                batch_texts, batch_ids = [], []

                for r in rows:
                    rid = r[0]
                    if table == "ojtdocument":
                        title = r[1] if len(r) > 1 else "TÃ i liá»‡u"
                        url = r[2] if len(r) > 2 else ""
                        print(f"  ğŸ“¥ TrÃ­ch xuáº¥t PDF/Word: {title}")
                        content = get_text_from_drive(url)
                        final_text = f"TÃ€I LIá»†U OJT: {title}. Ná»˜I DUNG: {content}. Link: {url}"
                    else:
                        final_text = r[1]
                    
                    batch_texts.append(final_text)
                    batch_ids.append(rid)

                # Batch 5 dÃ²ng Ä‘á»ƒ tá»‘i Æ°u Quota
                sub_batch_size = 5
                for i in range(0, len(batch_texts), sub_batch_size):
                    s_texts = batch_texts[i : i + sub_batch_size]
                    s_ids = batch_ids[i : i + sub_batch_size]
                    
                    print(f"ğŸ“¡ Äang gá»­i batch {i//sub_batch_size + 1} lÃªn Vertex AI...")
                    vectors = get_embeddings_batch(s_texts)
                    
                    if vectors:
                        for idx, vec in enumerate(vectors):
                            cur.execute(f'UPDATE "{table}" SET embedding = %s, last_content_indexed = %s WHERE "{sc["id_col"]}" = %s', 
                                       (vec, s_texts[idx], s_ids[idx]))
                        conn.commit()
                        print(f"  âœ… ÄÃ£ lÆ°u {len(vectors)} dÃ²ng. Nghá»‰ 5s...")
                        time.sleep(5)
            else:
                print(f"âœ… Báº£ng [{table}]: ÄÃ£ Ä‘á»“ng bá»™.")

        print("ğŸ‰ [System] HoÃ n táº¥t Ä‘á»“ng bá»™ toÃ n bá»™ dá»¯ liá»‡u.")
    except Exception as e:
        print(f"âŒ Lá»—i Sync: {e}"); conn.rollback() if conn else None
    finally:
        if conn: conn.close()

# ==================== 4. CORE RAG LOGIC ====================

def search_vectors(question, limit=7):
    query_vector = get_query_embedding(question)
    if not query_vector: return ""
    conn = None
    try:
        conn = psycopg2.connect(dsn=DB_DSN)
        cur = conn.cursor()
        results = []
        for t in ["ojtdocument", "job_position", "company", "semester"]:
            cur.execute(f'SELECT last_content_indexed, 1 - (embedding <=> %s::vector) as score FROM "{t}" WHERE embedding IS NOT NULL ORDER BY score DESC LIMIT 3', (query_vector,))
            for r in cur.fetchall():
                if r[1] > 0.18: 
                    results.append(f"[{t.upper()}] {r[0]}")
        return "\n".join(results)
    finally:
        if conn: conn.close()

def run_agent(question: str, file_content: str = None):
    from rag_core import start_chat_session, get_chat_response
    import re
    import psycopg2
    
    clean_question = question
    # 1. AI Refiner: Bung viáº¿t táº¯t (tt, mssv) nhÆ°ng giá»¯ nguyÃªn tÃªn riÃªng
    abbr_patterns = [r'\btt\b', r'\bmssv\b', r'\bmÃ´ mÃ´\b']
    if len(question.split()) < 5 or any(re.search(p, question.lower()) for p in abbr_patterns):
        refine_p = f"Chuáº©n hÃ³a cÃ¢u há»i: '{question}'. Bung viáº¿t táº¯t (tt=thá»±c táº­p, mssv=mÃ£ sá»‘ sinh viÃªn). Giá»¯ nguyÃªn tÃªn riÃªng. Chá»‰ tráº£ vá» cÃ¢u Ä‘Ã£ sá»­a."
        try:
            clean_question = start_chat_session().send_message(refine_p).text.strip()
            print(f"ğŸ” [Refine] {question} -> {clean_question}")
        except:
            clean_question = question

    # 2. Láº¥y Context tá»« Vector Search (Truy váº¥n Ä‘a báº£ng: job, company, user...)
    db_context = search_vectors(clean_question)
    
    # 3. Xá»­ lÃ½ Ä‘á»c ná»™i dung Link Drive trá»±c tiáº¿p (Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n tÃ i liá»‡u OJT)
    drive_content = ""
    target_link = ""
    # TÃ¬m xem trong db_context cÃ³ chá»©a link ojtdocument khÃ´ng
    if "[OJTDOCUMENT]" in db_context.upper():
        # TrÃ­ch xuáº¥t link drive tá»« context báº±ng Regex
        link_match = re.search(r'https://drive\.google\.com/[^\s]+', db_context)
        if link_match:
            target_link = link_match.group(0)
            print(f"ğŸ“‚ AI Ä‘ang truy cáº­p trá»±c tiáº¿p link Ä‘á»ƒ láº¥y ná»™i dung chi tiáº¿t: {target_link}")
            drive_content = get_text_from_drive(target_link)

    # 4. XÃ¢y dá»±ng Prompt tá»•ng há»£p
    final_prompt = f"""
    Dá»® LIá»†U Há»† THá»NG (Báº®T BUá»˜C Sá»¬ Dá»¤NG):
    {db_context if db_context else "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u liÃªn quan trong DB."}
    
    Ná»˜I DUNG Äá»ŒC TRá»°C TIáº¾P Tá»ª LINK DRIVE (Náº¾U CÃ“):
    {drive_content if drive_content else "KhÃ´ng cÃ³ ná»™i dung bá»• sung tá»« link."}
    
    FILE NGÆ¯á»œI DÃ™NG Táº¢I LÃŠN (Náº¾U CÃ“): 
    {file_content if file_content else "N/A"}
    
    CÃ‚U Há»I: {clean_question}
    
    YÃŠU Cáº¦U:
    - Æ¯U TIÃŠN sá»­ dá»¥ng 'Ná»˜I DUNG Äá»ŒC TRá»°C TIáº¾P Tá»ª LINK DRIVE' Ä‘á»ƒ tráº£ lá»i chi tiáº¿t cÃ¡c quy Ä‘á»‹nh OJT.
    - Sá»­ dá»¥ng 'Dá»® LIá»†U Há»† THá»NG' Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c thÃ´ng tin cÃ´ng ty, Ä‘á»‹a chá»‰, lÆ°Æ¡ng, hoáº·c thÃ´ng tin sinh viÃªn.
    - TrÃ¬nh bÃ y cÃ¢u tráº£ lá»i chuyÃªn nghiá»‡p, rÃµ rÃ ng tá»«ng Ã½.
    - PHáº¦N QUAN TRá»ŒNG Vá»€ LINK: 
       - Cuá»‘i cÃ¢u tráº£ lá»i, chá»‰ hiá»ƒn thá»‹ má»™t danh sÃ¡ch duy nháº¥t cÃ¡c 'Link tÃ i liá»‡u tham kháº£o'.
       - Tuyá»‡t Ä‘á»‘i KHÃ”NG liá»‡t kÃª láº·p láº¡i cÃ¹ng má»™t Ä‘Æ°á»ng link.
       - Náº¿u Link tá»« dá»¯ liá»‡u há»‡ thá»‘ng vÃ  Link tá»« ná»™i dung trá»±c tiáº¿p lÃ  má»™t, chá»‰ Ä‘Æ°á»£c hiá»ƒn thá»‹ 1 láº§n duy nháº¥t.
    """
    
    print(f"--- DEBUG CONTEXT SENT TO AI ---\n{db_context}\n-------------------------------")
    
    chat_session = start_chat_session()
    return get_chat_response(chat_session, final_prompt), "Mode: Hybrid Real-time RAG"


def run_cv_review(cv_text: str, user_message: str):
    from rag_core import start_chat_session
    context = search_vectors(cv_text)
    prompt = f"CV SINH VIÃŠN: {cv_text[:3000]}\n\nNGá»® Cáº¢NH Há»† THá»NG: {context}\n\nYÃŠU Cáº¦U: {user_message}\n\nHÆ¯á»šNG DáºªN: ÄÃ¡nh giÃ¡ Ä‘á»™ phÃ¹ há»£p CV vá»›i Job vÃ  Quy Ä‘á»‹nh OJT."
    return start_chat_session().send_message(prompt).text, "Mode: CV Reviewer Intelligence"

def check_vector_coverage():
    conn = psycopg2.connect(dsn=DB_DSN)
    cur = conn.cursor()
    for t in ["job_position", "ojtdocument", "User", "company"]:
        cur.execute(f'SELECT COUNT(*), COUNT(embedding) FROM "{t}"')
        total, has_v = cur.fetchone()
        print(f"ğŸ“Š {t}: {has_v}/{total} vectors.")
    conn.close()