import os
import re
import io
import time
import requests
import psycopg2
from psycopg2 import pool
import pdfplumber
import docx
from contextlib import contextmanager
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_fixed

load_dotenv()

# ==================== 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ====================

# 1.1 C·∫•u h√¨nh Database & Connection Pool
LOCAL_DB_URL = "postgresql://postgres:123@caboose.proxy.rlwy.net:54173/railway"
DB_DSN = os.environ.get("DB_DSN", LOCAL_DB_URL)

db_pool = None
try:
    # T·∫°o b·ªÉ k·∫øt n·ªëi (Min 1, Max 10) ƒë·ªÉ tr√°nh m·ªü l·∫°i connection li√™n t·ª•c
    db_pool = psycopg2.pool.ThreadedConnectionPool(1, 10, dsn=DB_DSN)
    print("‚úÖ [DB] Connection Pool initialized.")
except Exception as e:
    print(f"‚ùå [DB] Pool Error: {e}")

# 1.2 C·∫•u h√¨nh AI Local (LAZY LOADING - QUAN TR·ªåNG CHO RENDER)
# Kh√¥ng t·∫£i model ngay l·∫≠p t·ª©c ƒë·ªÉ tr√°nh Timeout khi kh·ªüi ƒë·ªông
local_embedder = None

def get_embedder():
    """H√†m t·∫£i model 'l∆∞·ªùi' - Ch·ªâ t·∫£i khi c·∫ßn d√πng"""
    global local_embedder
    if local_embedder is None:
        print("‚è≥ [AI Local] ƒêang t·∫£i Model Embedding (MiniLM)...")
        from sentence_transformers import SentenceTransformer
        local_embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("‚úÖ [AI Local] Model ƒë√£ s·∫µn s√†ng!")
    return local_embedder

# ==================== 2. T·ª™ ƒêI·ªÇN & H√ÄM B·ªî TR·ª¢ ====================

# T·ª´ ƒëi·ªÉn vi·∫øt t·∫Øt (Regex) - Nhanh h∆°n g·ªçi AI g·∫•p 1000 l·∫ßn
ABBREVIATIONS = {
    r"\btt\b": "th·ª±c t·∫≠p",
    r"\bojt\b": "th·ª±c t·∫≠p doanh nghi·ªáp",
    r"\bmssv\b": "m√£ s·ªë sinh vi√™n",
    r"\bcv\b": "h·ªì s∆° xin vi·ªác",
    r"\bcty\b": "c√¥ng ty",
    r"\bdn\b": "doanh nghi·ªáp",
    r"\bsem\b": "h·ªçc k·ª≥",
    r"\bjob\b": "vi·ªác l√†m",
    r"\bluong\b": "m·ª©c l∆∞∆°ng",
    r"\bhcm\b": "TP.HCM",
    r"\bhn\b": "H√† N·ªôi"
}

def quick_process_text(text):
    """Chu·∫©n h√≥a text si√™u t·ªëc b·∫±ng Regex"""
    if not text: return ""
    text = text.lower().strip()
    for pattern, replacement in ABBREVIATIONS.items():
        text = re.sub(pattern, replacement, text)
    return re.sub(r'\s+', ' ', text)

@contextmanager
def get_db_connection():
    """L·∫•y k·∫øt n·ªëi t·ª´ Pool an to√†n"""
    conn = None
    try:
        if db_pool:
            conn = db_pool.getconn()
            yield conn
        else:
            conn = psycopg2.connect(dsn=DB_DSN)
            yield conn
    except Exception as e:
        print(f"‚ùå DB Error: {e}")
        raise e
    finally:
        if conn and db_pool: db_pool.putconn(conn)
        elif conn: conn.close()

def get_text_from_drive(file_url):
    """T·∫£i v√† ƒë·ªçc n·ªôi dung file PDF/Word t·ª´ Google Drive"""
    if not file_url or "drive.google.com" not in file_url: return ""
    try:
        match = re.search(r'/d/([a-zA-Z0-9_-]+)', file_url) or re.search(r'id=([a-zA-Z0-9_-]+)', file_url)
        if not match: return ""
        url = f"https://drive.google.com/uc?export=download&id={match.group(1)}"
        
        # Timeout 10s ƒë·ªÉ kh√¥ng treo server
        res = requests.get(url, timeout=10)
        if res.status_code == 200:
            stream = io.BytesIO(res.content)
            try:
                # ∆Øu ti√™n ƒë·ªçc PDF
                with pdfplumber.open(stream) as pdf:
                    return " ".join([p.extract_text() or "" for p in pdf.pages[:5]])
            except: 
                # Fallback sang Word
                stream.seek(0)
                doc = docx.Document(stream)
                return " ".join([p.text for p in doc.paragraphs])
    except: pass
    return ""

# ==================== 3. H√ÄM VECTOR LOCAL (ƒê√É S·ª¨A LAZY LOAD) ====================

def get_embeddings_batch(texts):
    """T·∫°o Vector b·∫±ng CPU Server (Free & Fast)"""
    embedder = get_embedder() # <--- G·ªçi h√†m lazy load
    if not embedder or not texts: return []
    
    # C·∫Øt ng·∫Øn text ƒë·ªÉ tr√°nh l·ªói model limit
    clean_texts = [str(t).replace("\n", " ").strip()[:1000] for t in texts if t]
    try:
        embeddings = embedder.encode(clean_texts)
        return embeddings.tolist()
    except Exception as e:
        print(f"‚ö†Ô∏è Local Embed Error: {e}")
        return []

def get_query_embedding(text):
    """T·∫°o vector cho 1 c√¢u h·ªèi"""
    embedder = get_embedder() # <--- G·ªçi h√†m lazy load
    try:
        embedding = embedder.encode(text)
        return embedding.tolist()
    except: return None

# ==================== 4. SEARCH ENGINE (T·ªêI ∆ØU H√ìA) ====================

def search_vectors(question):
    t0 = time.time()
    
    # 1. T·∫°o vector c√¢u h·ªèi (Local)
    query_vector = get_query_embedding(question)
    if not query_vector: return ""
    
    results = []
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # 2. Query t·ªëi ∆∞u (G·ªôp 9 b·∫£ng)
                # L∆∞u √Ω: DB Vector c·ªôt ph·∫£i l√† vector(384)
                sql_query = """
                    (SELECT 'T√ÄI LI·ªÜU', last_content_indexed, (embedding <=> %s::vector) as d FROM ojtdocument WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 3)
                    UNION ALL
                    (SELECT 'VI·ªÜC L√ÄM', last_content_indexed, (embedding <=> %s::vector) as d FROM job_position WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 4)
                    UNION ALL
                    (SELECT 'DOANH NGHI·ªÜP', last_content_indexed, (embedding <=> %s::vector) as d FROM company WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'H·ªí S∆† SV', last_content_indexed, (embedding <=> %s::vector) as d FROM "User" WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'NG√ÄNH H·ªåC', last_content_indexed, (embedding <=> %s::vector) as d FROM major WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'H·ªåC K·ª≤', last_content_indexed, (embedding <=> %s::vector) as d FROM semester WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 1)
                    UNION ALL
                    (SELECT 'DOC C√îNG TY', last_content_indexed, (embedding <=> %s::vector) as d FROM companydocument WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'FEEDBACK', last_content_indexed, (embedding <=> %s::vector) as d FROM finalreport WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'TH·ªêNG K√ä', last_content_indexed, (embedding <=> %s::vector) as d FROM job_title_overview WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    ORDER BY d ASC LIMIT 12
                """
                # Truy·ªÅn tham s·ªë 9 l·∫ßn cho 9 d·∫•u %s
                params = (query_vector,) * 9 
                cur.execute(sql_query, params)
                
                for r in cur.fetchall():
                    # L·∫•y k·∫øt qu·∫£ c√≥ kho·∫£ng c√°ch < 0.85 (ƒê√£ n·ªõi l·ªèng ƒë·ªÉ l·∫•y nhi·ªÅu d·ªØ li·ªáu h∆°n)
                    if r[2] < 0.85: 
                        results.append(f"[{r[0]}] {r[1]}")
                        
    except Exception as e:
        print(f"‚ùå Search Error: {e}")
    
    print(f"‚ö° Local Search: {time.time() - t0:.3f}s")
    return "\n\n".join(results)

# ==================== 5. CORE RAG LOGIC ====================

def run_agent(question: str, file_content: str = None):
    # Import c·ª•c b·ªô ƒë·ªÉ tr√°nh l·ªói circular import
    from rag_core import start_chat_session, get_chat_response
    
    t_start = time.time()
    
    # 1. X·ª≠ l√Ω c√¢u h·ªèi
    clean_question = quick_process_text(question)
    print(f"üßπ Input: '{question}' -> '{clean_question}'")
    
    # 2. T√¨m ki·∫øm Vector
    db_context = search_vectors(clean_question) 
    
    # 3. K√çCH HO·∫†T ƒê·ªåC FILE DRIVE TR·ª∞C TI·∫æP (QUAN TR·ªåNG)
    realtime_file_content = ""
    source_link = ""
    
    if "drive.google.com" in db_context:
        link_match = re.search(r'https://drive\.google\.com/[^\s]+', db_context)
        if link_match:
            target_url = link_match.group(0).rstrip(").,")
            print(f"üöÄ [Real-time] ƒêang ƒë·ªçc chi ti·∫øt: {target_url}")
            
            realtime_file_content = get_text_from_drive(target_url)
            
            if realtime_file_content:
                source_link = target_url
                print(f"   ‚úÖ ƒê√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(realtime_file_content)} k√Ω t·ª± chi ti·∫øt.")

    # 4. T·∫°o Prompt
    final_prompt = f"""
    VAI TR√í: Tr·ª£ l√Ω tuy·ªÉn d·ª•ng v√† ƒë√†o t·∫°o OJT chuy√™n nghi·ªáp.

    D·ªÆ LI·ªÜU T√ìM T·∫ÆT T·ª™ H·ªÜ TH·ªêNG:
    {db_context}
    
    --------------------------------------------------
    N·ªòI DUNG CHI TI·∫æT ƒê·∫¶Y ƒê·ª¶ T·ª™ T√ÄI LI·ªÜU (∆ØU TI√äN D√ôNG C√ÅI N√ÄY):
    {realtime_file_content if realtime_file_content else "Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt file."}
    --------------------------------------------------
    
    FILE NG∆Ø·ªúI D√ôNG T·∫¢I L√äN (N·∫æU C√ì):
    {file_content if file_content else "N/A"}
    
    C√ÇU H·ªéI: {clean_question}
    
    Y√äU C·∫¶U TR·∫¢ L·ªúI: 
    1. D·ª±a v√†o 'N·ªòI DUNG CHI TI·∫æT', h√£y tr√≠ch xu·∫•t to√†n b·ªô th√¥ng tin quan tr·ªçng:
       - Gi·ªõi thi·ªáu c√¥ng ty.
       - V·ªã tr√≠ tuy·ªÉn d·ª•ng & Y√™u c·∫ßu k·ªπ nƒÉng.
       - Quy·ªÅn l·ª£i (L∆∞∆°ng, tr·ª£ c·∫•p, m√¥i tr∆∞·ªùng).
       - C√°ch th·ª©c ·ª©ng tuy·ªÉn (Email, Quy tr√¨nh).
    2. Tr√¨nh b√†y r√µ r√†ng, g·∫°ch ƒë·∫ßu d√≤ng.
    3. N·∫øu c√≥ link t√†i li·ªáu g·ªëc ({source_link}), H√ÉY ƒê·ªÇ N√ì ·ªû CU·ªêI C√ôNG.
    """
    
    try:
        chat_session = start_chat_session()
        answer = get_chat_response(chat_session, final_prompt)
    except Exception as e:
        answer = "‚ö†Ô∏è H·ªá th·ªëng ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau."
        print(f"‚ùå Chat Error: {e}")
    
    print(f"‚è±Ô∏è Total Time: {time.time() - t_start:.3f}s")
    mode_label = "RAG + Realtime" if realtime_file_content else "RAG Fast"
    return answer, mode_label

# ==================== 6. ƒê·ªíNG B·ªò D·ªÆ LI·ªÜU (SYNC ALL) ====================

def sync_all_data(force_reset=False):
    print(f"üîÑ [Sync] B·∫Øt ƒë·∫ßu ƒë·ªìng b·ªô d·ªØ li·ªáu (Local Embedding)...")
    t_start = time.time()
    
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                if force_reset:
                    print("‚ö†Ô∏è [Reset] ƒêang x√≥a vector c≈©...")
                    tables = ["job_position", "company", "semester", "User", "major", "ojtdocument", "companydocument", "finalreport", "job_title_overview"]
                    for t in tables:
                        # Ki·ªÉm tra b·∫£ng t·ªìn t·∫°i
                        cur.execute(f"SELECT to_regclass('public.\"{t}\"');")
                        if cur.fetchone()[0]:
                            cur.execute(f'UPDATE "{t}" SET embedding = NULL, last_content_indexed = NULL;')
                    conn.commit()

                # --- ƒê·ªäNH NGHƒ®A K·ªäCH B·∫¢N ---
                scenarios = [
                    # 1. Job
                    {"table": "job_position", "id": "job_position_id", "sql": """
                        SELECT jp.job_position_id, 'V·ªä TR√ç: ' || COALESCE(jp.job_title, '') || '. C√îNG TY: ' || COALESCE(c.name, 'N/A') || '. L∆Ø∆†NG: ' || COALESCE(jp.salary_range, '') || '. M√î T·∫¢: ' || COALESCE(jd.job_description, '') || '. Y√äU C·∫¶U: ' || COALESCE(jp.requirements, '') as text 
                        FROM job_position jp 
                        LEFT JOIN semester_company sc ON jp.semester_company_id = sc.semester_company_id
                        LEFT JOIN company c ON sc.company_id = c.company_id
                        LEFT JOIN job_description jd ON jp.job_position_id = jd.job_position_id
                    """},
                    # 2. User
                    {"table": "User", "id": "user_id", "sql": """
                        SELECT u.user_id, 'NG∆Ø·ªúI D√ôNG: ' || COALESCE(u.fullname, '') || '. MSSV: ' || COALESCE(u.student_code, '') || '. EMAIL: ' || COALESCE(u.email, '') || '. NG√ÄNH: ' || COALESCE(m.major_title, '') || '. C√îNG TY: ' || COALESCE(c.name, '') as text
                        FROM "User" u
                        LEFT JOIN major m ON u.major_id = m.major_id
                        LEFT JOIN company c ON u.company_id = c.company_id
                    """},
                    # 3. Docs (C√≥ ƒë·ªçc file)
                    {"table": "ojtdocument", "id": "ojtdocument_id", "sql": "SELECT ojtdocument_id, title, file_url FROM ojtdocument"},
                    {"table": "companydocument", "id": "companydocument_id", "sql": """
                        SELECT cd.companydocument_id, 'DOC C√îNG TY: ' || COALESCE(c.name, '') || '. T√äN: ' || COALESCE(cd.title, '') as text, cd.file_url 
                        FROM companydocument cd LEFT JOIN semester_company sc ON cd.semester_company_id = sc.semester_company_id LEFT JOIN company c ON sc.company_id = c.company_id
                    """},
                    # 4. C√°c b·∫£ng ƒë∆°n l·∫ª kh√°c
                    {"table": "company", "id": "company_id", "sql": "SELECT company_id, 'C√îNG TY: ' || COALESCE(name, '') || '. ƒê·ªäA CH·ªà: ' || COALESCE(address, '') || '. EMAIL: ' || COALESCE(contact_email, '') as text FROM company"},
                    {"table": "semester", "id": "semester_id", "sql": "SELECT semester_id, 'H·ªåC K·ª≤: ' || COALESCE(name, '') || '. T·ª™: ' || COALESCE(start_date::text, '') || ' ƒê·∫æN: ' || COALESCE(end_date::text, '') as text FROM semester"},
                    {"table": "major", "id": "major_id", "sql": "SELECT major_id, 'NG√ÄNH: ' || COALESCE(major_title, '') || '. M√î T·∫¢: ' || COALESCE(description, '') as text FROM major"},
                    {"table": "finalreport", "id": "finalreport_id", "sql": """
                         SELECT fr.finalreport_id, 'ƒê√ÅNH GI√Å: SV ' || COALESCE(u.fullname, '') || ' T·∫†I ' || COALESCE(c.name, '') || '. ƒêI·ªÇM: ' || COALESCE(fr.company_rating::text, '0') || '. NH·∫¨N X√âT: ' || COALESCE(fr.company_feedback, '') as text
                         FROM finalreport fr LEFT JOIN "User" u ON fr.user_id = u.user_id LEFT JOIN job_position jp ON fr.job_position_id = jp.job_position_id LEFT JOIN semester_company sc ON jp.semester_company_id = sc.semester_company_id LEFT JOIN company c ON sc.company_id = c.company_id
                    """},
                    {"table": "job_title_overview", "id": "job_title_id", "sql": "SELECT job_title_id, 'TH·ªêNG K√ä VI·ªÜC L√ÄM: ' || COALESCE(job_title, '') || '. S·ªê L∆Ø·ª¢NG: ' || COALESCE(position_amount::text, '0') as text FROM job_title_overview"}
                ]

                # --- LOOP X·ª¨ L√ù ---
                for sc in scenarios:
                    table = sc['table']
                    id_col = sc['id']
                    
                    cur.execute(f"SELECT to_regclass('public.\"{table}\"');")
                    if not cur.fetchone()[0]: continue

                    # L·∫•y d·ªØ li·ªáu ch∆∞a index
                    cur.execute(f"""
                        WITH source AS ({sc['sql']})
                        SELECT s.* FROM source s JOIN "{table}" t ON s.{id_col} = t."{id_col}"
                        WHERE t.embedding IS NULL OR t.last_content_indexed IS NULL
                    """)
                    rows = cur.fetchall()
                    if not rows: continue
                    
                    print(f"üì¶ [{table}] X·ª≠ l√Ω {len(rows)} d√≤ng m·ªõi.")
                    BATCH_SIZE = 10
                    
                    for i in range(0, len(rows), BATCH_SIZE):
                        batch = rows[i : i + BATCH_SIZE]
                        batch_texts, batch_ids = [], []

                        for r in batch:
                            rid = r[0]
                            # X·ª≠ l√Ω File Drive
                            if table in ["ojtdocument", "companydocument"]:
                                title = r[1]
                                url = r[2] if len(r) > 2 else ""
                                content = ""
                                if "drive.google.com" in url:
                                    print(f"   üì• ƒê·ªçc file: {title[:20]}...")
                                    content = get_text_from_drive(url)
                                final_text = f"{title}. CHI TI·∫æT: {content}. Link: {url}"
                            else:
                                final_text = r[1]
                            
                            batch_texts.append(final_text)
                            batch_ids.append(rid)

                        # T·∫°o Embedding LOCAL
                        vectors = get_embeddings_batch(batch_texts)
                        
                        # L∆∞u v√†o DB
                        if vectors:
                            for idx, vec in enumerate(vectors):
                                cur.execute(f'UPDATE "{table}" SET embedding = %s, last_content_indexed = %s WHERE "{id_col}" = %s', 
                                            (vec, batch_texts[idx], batch_ids[idx]))
                            conn.commit()
                            print(f"   ‚úÖ Saved batch {i//BATCH_SIZE + 1} (Local Vector).")

        print(f"üéâ [Sync] Ho√†n t·∫•t sau {time.time() - t_start:.2f}s.")
    except Exception as e:
        print(f"‚ùå L·ªói Sync: {e}")

# ==================== 7. CV REVIEW (MATCH MAKER) ====================

def run_cv_review(cv_text: str, user_message: str):
    from rag_core import start_chat_session, get_chat_response
    
    # 1. Debug
    print(f"üìÑ [CV Review] Length: {len(cv_text)} chars.")
    if len(cv_text) < 100:
        return "‚ö†Ô∏è L·ªói: Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c CV (File ·∫£nh ho·∫∑c l·ªói).", "CV Error"

    # 2. Search Job
    search_query = cv_text[:500] + " tuy·ªÉn d·ª•ng vi·ªác l√†m k·ªπ nƒÉng"
    context = search_vectors(search_query)
    
    # 3. Match Prompt
    prompt = f"""
    VAI TR√í: Chuy√™n gia HR Tech.

    D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO:
    1. CV ·ª®NG VI√äN: {cv_text[:3000]}
    2. DANH S√ÅCH JOB: {context}
    3. Y√äU C·∫¶U: "{user_message}"
    
    NHI·ªÜM V·ª§:
    - B·ªé QUA c√°c file quy ƒë·ªãnh OJT. Ch·ªâ t·∫≠p trung v√†o VI·ªÜC L√ÄM.
    - So s√°nh k·ªπ nƒÉng trong CV v·ªõi Job.
    - ƒê∆∞a ra Top 3 Job ph√π h·ª£p nh·∫•t.
    
    ƒê·ªäNH D·∫†NG:
    üéØ **Top 1: [V·ªã Tr√≠] - [C√¥ng Ty]**
       - ‚úÖ L√Ω do match: ...
       - ‚ö†Ô∏è C·∫ßn b·ªï sung: ...
    """
    
    print("ü§ñ [CV Review] Matching...")
    return get_chat_response(start_chat_session(), prompt), "CV Matcher"
