import os
import re
import io
import time
import requests
import psycopg2
from psycopg2 import pool
import pdfplumber
import docx
from contextlib import contextmanager
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_fixed

load_dotenv()

# ==================== 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ====================

# 1.1 C·∫•u h√¨nh Database & Connection Pool
LOCAL_DB_URL = "postgresql://postgres:123@caboose.proxy.rlwy.net:54173/railway"
DB_DSN = os.environ.get("DB_DSN", LOCAL_DB_URL)

db_pool = None
try:
    # T·∫°o b·ªÉ k·∫øt n·ªëi (Min 1, Max 10) ƒë·ªÉ tr√°nh m·ªü l·∫°i connection li√™n t·ª•c
    db_pool = psycopg2.pool.ThreadedConnectionPool(1, 10, dsn=DB_DSN)
    print("‚úÖ [DB] Connection Pool initialized.")
except Exception as e:
    print(f"‚ùå [DB] Pool Error: {e}")

# 1.2 C·∫•u h√¨nh AI Local (LAZY LOADING - QUAN TR·ªåNG CHO RENDER)
# Kh√¥ng t·∫£i model ngay l·∫≠p t·ª©c ƒë·ªÉ tr√°nh Timeout khi kh·ªüi ƒë·ªông
local_embedder = None

def get_embedder():
    """H√†m t·∫£i model 'l∆∞·ªùi' - Ch·ªâ t·∫£i khi c·∫ßn d√πng"""
    global local_embedder
    if local_embedder is None:
        print("‚è≥ [AI Local] ƒêang t·∫£i Model Embedding (MiniLM)...")
        from sentence_transformers import SentenceTransformer
        local_embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("‚úÖ [AI Local] Model ƒë√£ s·∫µn s√†ng!")
    return local_embedder

# ==================== 2. T·ª™ ƒêI·ªÇN & H√ÄM B·ªî TR·ª¢ ====================

# T·ª´ ƒëi·ªÉn vi·∫øt t·∫Øt (Regex) - Nhanh h∆°n g·ªçi AI g·∫•p 1000 l·∫ßn
ABBREVIATIONS = {
    r"\btt\b": "th·ª±c t·∫≠p",
    r"\bojt\b": "th·ª±c t·∫≠p doanh nghi·ªáp",
    r"\bmssv\b": "m√£ s·ªë sinh vi√™n",
    r"\bcv\b": "h·ªì s∆° xin vi·ªác",
    r"\bcty\b": "c√¥ng ty",
    r"\bdn\b": "doanh nghi·ªáp",
    r"\bsem\b": "h·ªçc k·ª≥",
    r"\bjob\b": "vi·ªác l√†m",
    r"\bluong\b": "m·ª©c l∆∞∆°ng",
    r"\bhcm\b": "TP.HCM",
    r"\bhn\b": "H√† N·ªôi"
}

def quick_process_text(text):
    """Chu·∫©n h√≥a text si√™u t·ªëc b·∫±ng Regex"""
    if not text: return ""
    text = text.lower().strip()
    for pattern, replacement in ABBREVIATIONS.items():
        text = re.sub(pattern, replacement, text)
    return re.sub(r'\s+', ' ', text)

@contextmanager
def get_db_connection():
    """L·∫•y k·∫øt n·ªëi t·ª´ Pool an to√†n"""
    conn = None
    try:
        if db_pool:
            conn = db_pool.getconn()
            yield conn
        else:
            conn = psycopg2.connect(dsn=DB_DSN)
            yield conn
    except Exception as e:
        print(f"‚ùå DB Error: {e}")
        raise e
    finally:
        if conn and db_pool: db_pool.putconn(conn)
        elif conn: conn.close()

def get_text_from_drive(file_url):
    """T·∫£i v√† ƒë·ªçc n·ªôi dung file PDF/Word t·ª´ Google Drive"""
    if not file_url or "drive.google.com" not in file_url: return ""
    try:
        match = re.search(r'/d/([a-zA-Z0-9_-]+)', file_url) or re.search(r'id=([a-zA-Z0-9_-]+)', file_url)
        if not match: return ""
        url = f"https://drive.google.com/uc?export=download&id={match.group(1)}"
        
        # Timeout 10s ƒë·ªÉ kh√¥ng treo server
        res = requests.get(url, timeout=10)
        if res.status_code == 200:
            stream = io.BytesIO(res.content)
            try:
                # ∆Øu ti√™n ƒë·ªçc PDF
                with pdfplumber.open(stream) as pdf:
                    return " ".join([p.extract_text() or "" for p in pdf.pages[:5]])
            except: 
                # Fallback sang Word
                stream.seek(0)
                doc = docx.Document(stream)
                return " ".join([p.text for p in doc.paragraphs])
    except: pass
    return ""

# ==================== 3. H√ÄM VECTOR LOCAL (ƒê√É S·ª¨A LAZY LOAD) ====================

def get_embeddings_batch(texts):
    """T·∫°o Vector b·∫±ng CPU Server (Free & Fast)"""
    embedder = get_embedder() # <--- G·ªçi h√†m lazy load
    if not embedder or not texts: return []
    
    # C·∫Øt ng·∫Øn text ƒë·ªÉ tr√°nh l·ªói model limit
    clean_texts = [str(t).replace("\n", " ").strip()[:1000] for t in texts if t]
    try:
        embeddings = embedder.encode(clean_texts)
        return embeddings.tolist()
    except Exception as e:
        print(f"‚ö†Ô∏è Local Embed Error: {e}")
        return []

def get_query_embedding(text):
    """T·∫°o vector cho 1 c√¢u h·ªèi"""
    embedder = get_embedder() # <--- G·ªçi h√†m lazy load
    try:
        embedding = embedder.encode(text)
        return embedding.tolist()
    except: return None

# ==================== 4. SEARCH ENGINE (T·ªêI ∆ØU H√ìA) ====================

def search_vectors(question):
    t0 = time.time()
    
    # 1. T·∫°o vector c√¢u h·ªèi (Local)
    query_vector = get_query_embedding(question)
    if not query_vector: return ""
    
    results = []
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # 2. Query t·ªëi ∆∞u (G·ªôp 9 b·∫£ng)
                # L∆∞u √Ω: DB Vector c·ªôt ph·∫£i l√† vector(384)
                sql_query = """
                    (SELECT 'T√ÄI LI·ªÜU', last_content_indexed, (embedding <=> %s::vector) as d FROM ojtdocument WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 3)
                    UNION ALL
                    (SELECT 'VI·ªÜC L√ÄM', last_content_indexed, (embedding <=> %s::vector) as d FROM job_position WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 4)
                    UNION ALL
                    (SELECT 'DOANH NGHI·ªÜP', last_content_indexed, (embedding <=> %s::vector) as d FROM company WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'H·ªí S∆† SV', last_content_indexed, (embedding <=> %s::vector) as d FROM "User" WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'NG√ÄNH H·ªåC', last_content_indexed, (embedding <=> %s::vector) as d FROM major WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'H·ªåC K·ª≤', last_content_indexed, (embedding <=> %s::vector) as d FROM semester WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 1)
                    UNION ALL
                    (SELECT 'DOC C√îNG TY', last_content_indexed, (embedding <=> %s::vector) as d FROM companydocument WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'FEEDBACK', last_content_indexed, (embedding <=> %s::vector) as d FROM finalreport WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    UNION ALL
                    (SELECT 'TH·ªêNG K√ä', last_content_indexed, (embedding <=> %s::vector) as d FROM job_title_overview WHERE embedding IS NOT NULL ORDER BY d ASC LIMIT 2)
                    ORDER BY d ASC LIMIT 12
                """
                # Truy·ªÅn tham s·ªë 9 l·∫ßn cho 9 d·∫•u %s
                params = (query_vector,) * 9 
                cur.execute(sql_query, params)
                
                for r in cur.fetchall():
                    # L·∫•y k·∫øt qu·∫£ c√≥ kho·∫£ng c√°ch < 0.85 (ƒê√£ n·ªõi l·ªèng ƒë·ªÉ l·∫•y nhi·ªÅu d·ªØ li·ªáu h∆°n)
                    if r[2] < 0.85: 
                        results.append(f"[{r[0]}] {r[1]}")
                        
    except Exception as e:
        print(f"‚ùå Search Error: {e}")
    
    print(f"‚ö° Local Search: {time.time() - t0:.3f}s")
    return "\n\n".join(results)

# ==================== 5. CORE RAG LOGIC ====================

def run_agent(question: str, file_content: str = None):
    # Import c·ª•c b·ªô ƒë·ªÉ tr√°nh l·ªói circular import
    from rag_core import start_chat_session, get_chat_response
    
    t_start = time.time()
    
    # 1. X·ª≠ l√Ω c√¢u h·ªèi
    clean_question = quick_process_text(question)
    print(f"üßπ Input: '{question}' -> '{clean_question}'")
    
    # 2. T√¨m ki·∫øm Vector
    db_context = search_vectors(clean_question) 
    
    # 3. K√çCH HO·∫†T ƒê·ªåC FILE DRIVE TR·ª∞C TI·∫æP (QUAN TR·ªåNG)
    realtime_file_content = ""
    source_link = ""
    
    if "drive.google.com" in db_context:
        link_match = re.search(r'https://drive\.google\.com/[^\s]+', db_context)
        if link_match:
            target_url = link_match.group(0).rstrip(").,")
            print(f"üöÄ [Real-time] ƒêang ƒë·ªçc chi ti·∫øt: {target_url}")
            
            realtime_file_content = get_text_from_drive(target_url)
            
            if realtime_file_content:
                source_link = target_url
                print(f"   ‚úÖ ƒê√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(realtime_file_content)} k√Ω t·ª± chi ti·∫øt.")

    # 4. T·∫°o Prompt
    final_prompt = f"""
    VAI TR√í: Tr·ª£ l√Ω tuy·ªÉn d·ª•ng v√† ƒë√†o t·∫°o OJT chuy√™n nghi·ªáp.

    D·ªÆ LI·ªÜU T√ìM T·∫ÆT T·ª™ H·ªÜ TH·ªêNG:
    {db_context}
    
    --------------------------------------------------
    N·ªòI DUNG CHI TI·∫æT ƒê·∫¶Y ƒê·ª¶ T·ª™ T√ÄI LI·ªÜU (∆ØU TI√äN D√ôNG C√ÅI N√ÄY):
    {realtime_file_content if realtime_file_content else "Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung chi ti·∫øt file."}
    --------------------------------------------------
    
    FILE NG∆Ø·ªúI D√ôNG T·∫¢I L√äN (N·∫æU C√ì):
    {file_content if file_content else "N/A"}
    
    C√ÇU H·ªéI: {clean_question}
    
    Y√äU C·∫¶U TR·∫¢ L·ªúI: 
    1. D·ª±a v√†o 'N·ªòI DUNG CHI TI·∫æT', h√£y tr√≠ch xu·∫•t to√†n b·ªô th√¥ng tin quan tr·ªçng:
       - Gi·ªõi thi·ªáu c√¥ng ty.
       - V·ªã tr√≠ tuy·ªÉn d·ª•ng & Y√™u c·∫ßu k·ªπ nƒÉng.
       - Quy·ªÅn l·ª£i (L∆∞∆°ng, tr·ª£ c·∫•p, m√¥i tr∆∞·ªùng).
       - C√°ch th·ª©c ·ª©ng tuy·ªÉn (Email, Quy tr√¨nh).
    2. Tr√¨nh b√†y r√µ r√†ng, g·∫°ch ƒë·∫ßu d√≤ng.
    3. N·∫øu c√≥ link t√†i li·ªáu g·ªëc ({source_link}), H√ÉY ƒê·ªÇ N√ì ·ªû CU·ªêI C√ôNG.
    """
    
    try:
        chat_session = start_chat_session()
        answer = get_chat_response(chat_session, final_prompt)
    except Exception as e:
        answer = "‚ö†Ô∏è H·ªá th·ªëng ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau."
        print(f"‚ùå Chat Error: {e}")
    
    print(f"‚è±Ô∏è Total Time: {time.time() - t_start:.3f}s")
    mode_label = "RAG + Realtime" if realtime_file_content else "RAG Fast"
    return answer, mode_label

# ==================== 6. ƒê·ªíNG B·ªò D·ªÆ LI·ªÜU (SYNC ALL) ====================
SYNC_STATE = {
    "is_running": False,
    "step": "S·∫µn s√†ng",
    "detail": "",
    "processed": 0,
    "total_estimate": 0
}
def sync_all_data(force_reset=False):
    global SYNC_STATE
    
    print(f"üîÑ [Sync] B·∫Øt ƒë·∫ßu ƒë·ªìng b·ªô d·ªØ li·ªáu...")
    # C·∫≠p nh·∫≠t tr·∫°ng th√°i b·∫Øt ƒë·∫ßu
    SYNC_STATE["is_running"] = True
    SYNC_STATE["step"] = "ƒêang kh·ªüi ƒë·ªông..."
    SYNC_STATE["processed"] = 0
    
    t_start = time.time()
    
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                if force_reset:
                    SYNC_STATE["step"] = "ƒêang x√≥a d·ªØ li·ªáu c≈© (Reset)..."
                    print("‚ö†Ô∏è [Reset] ƒêang x√≥a vector c≈©...")
                    tables = ["job_position", "company", "semester", "User", "major", "ojtdocument", "companydocument", "finalreport", "job_title_overview"]
                    for t in tables:
                        cur.execute(f"SELECT to_regclass('public.\"{t}\"');")
                        if cur.fetchone()[0]:
                            cur.execute(f'UPDATE "{t}" SET embedding = NULL, last_content_indexed = NULL;')
                    conn.commit()

                # --- ƒê·ªäNH NGHƒ®A K·ªäCH B·∫¢N ---
                scenarios = [
                    # 1. Job
                    {"table": "job_position", "id": "job_position_id", "sql": """
                        SELECT jp.job_position_id, 'V·ªä TR√ç: ' || COALESCE(jp.job_title, '') || '. C√îNG TY: ' || COALESCE(c.name, 'N/A') || '. L∆Ø∆†NG: ' || COALESCE(jp.salary_range, '') || '. M√î T·∫¢: ' || COALESCE(jd.job_description, '') || '. Y√äU C·∫¶U: ' || COALESCE(jp.requirements, '') as text 
                        FROM job_position jp 
                        LEFT JOIN semester_company sc ON jp.semester_company_id = sc.semester_company_id
                        LEFT JOIN company c ON sc.company_id = c.company_id
                        LEFT JOIN job_description jd ON jp.job_position_id = jd.job_position_id
                    """},
                    # 2. User
                    {"table": "User", "id": "user_id", "sql": """
                        SELECT u.user_id, 'NG∆Ø·ªúI D√ôNG: ' || COALESCE(u.fullname, '') || '. MSSV: ' || COALESCE(u.student_code, '') || '. EMAIL: ' || COALESCE(u.email, '') || '. NG√ÄNH: ' || COALESCE(m.major_title, '') || '. C√îNG TY: ' || COALESCE(c.name, '') as text
                        FROM "User" u
                        LEFT JOIN major m ON u.major_id = m.major_id
                        LEFT JOIN company c ON u.company_id = c.company_id
                    """},
                    # 3. Docs (C√≥ ƒë·ªçc file)
                    {"table": "ojtdocument", "id": "ojtdocument_id", "sql": "SELECT ojtdocument_id, title, file_url FROM ojtdocument"},
                    {"table": "companydocument", "id": "companydocument_id", "sql": """
                        SELECT cd.companydocument_id, 'DOC C√îNG TY: ' || COALESCE(c.name, '') || '. T√äN: ' || COALESCE(cd.title, '') as text, cd.file_url 
                        FROM companydocument cd LEFT JOIN semester_company sc ON cd.semester_company_id = sc.semester_company_id LEFT JOIN company c ON sc.company_id = c.company_id
                    """},
                    # 4. C√°c b·∫£ng ƒë∆°n l·∫ª kh√°c
                    {"table": "company", "id": "company_id", "sql": "SELECT company_id, 'C√îNG TY: ' || COALESCE(name, '') || '. ƒê·ªäA CH·ªà: ' || COALESCE(address, '') || '. EMAIL: ' || COALESCE(contact_email, '') as text FROM company"},
                    {"table": "semester", "id": "semester_id", "sql": "SELECT semester_id, 'H·ªåC K·ª≤: ' || COALESCE(name, '') || '. T·ª™: ' || COALESCE(start_date::text, '') || ' ƒê·∫æN: ' || COALESCE(end_date::text, '') as text FROM semester"},
                    {"table": "major", "id": "major_id", "sql": "SELECT major_id, 'NG√ÄNH: ' || COALESCE(major_title, '') || '. M√î T·∫¢: ' || COALESCE(description, '') as text FROM major"},
                    {"table": "finalreport", "id": "finalreport_id", "sql": """
                         SELECT fr.finalreport_id, 'ƒê√ÅNH GI√Å: SV ' || COALESCE(u.fullname, '') || ' T·∫†I ' || COALESCE(c.name, '') || '. ƒêI·ªÇM: ' || COALESCE(fr.company_rating::text, '0') || '. NH·∫¨N X√âT: ' || COALESCE(fr.company_feedback, '') as text
                         FROM finalreport fr LEFT JOIN "User" u ON fr.user_id = u.user_id LEFT JOIN job_position jp ON fr.job_position_id = jp.job_position_id LEFT JOIN semester_company sc ON jp.semester_company_id = sc.semester_company_id LEFT JOIN company c ON sc.company_id = c.company_id
                    """},
                    {"table": "job_title_overview", "id": "job_title_id", "sql": "SELECT job_title_id, 'TH·ªêNG K√ä VI·ªÜC L√ÄM: ' || COALESCE(job_title, '') || '. S·ªê L∆Ø·ª¢NG: ' || COALESCE(position_amount::text, '0') as text FROM job_title_overview"}
                ]


                # --- LOOP X·ª¨ L√ù (S·ª¨A ƒêO·∫†N N√ÄY ƒê·ªÇ B√ÅO C√ÅO TI·∫æN ƒê·ªò) ---
                for sc in scenarios:
                    table = sc['table']
                    id_col = sc['id']
                    
                    # Update tr·∫°ng th√°i: ƒêang qu√©t b·∫£ng n√†o
                    SYNC_STATE["step"] = f"ƒêang qu√©t b·∫£ng: {table}"
                    
                    cur.execute(f"SELECT to_regclass('public.\"{table}\"');")
                    if not cur.fetchone()[0]: continue

                    # L·∫•y d·ªØ li·ªáu ch∆∞a index
                    cur.execute(f"""
                        WITH source AS ({sc['sql']})
                        SELECT s.* FROM source s JOIN "{table}" t ON s.{id_col} = t."{id_col}"
                        WHERE t.embedding IS NULL OR t.last_content_indexed IS NULL
                    """)
                    rows = cur.fetchall()
                    
                    if not rows: continue
                    
                    print(f"üì¶ [{table}] X·ª≠ l√Ω {len(rows)} d√≤ng m·ªõi.")
                    BATCH_SIZE = 10
                    
                    for i in range(0, len(rows), BATCH_SIZE):
                        batch = rows[i : i + BATCH_SIZE]
                        batch_texts, batch_ids = [], []
                        
                        # --- C·∫¨P NH·∫¨T TI·∫æN ƒê·ªò CHI TI·∫æT ---
                        SYNC_STATE["step"] = f"ƒêang x·ª≠ l√Ω {table}"
                        SYNC_STATE["detail"] = f"Batch {i//BATCH_SIZE + 1} ({len(batch)} d√≤ng)"
                        SYNC_STATE["processed"] += len(batch)

                        for r in batch:
                            rid = r[0]
                            # X·ª≠ l√Ω File Drive
                            if table in ["ojtdocument", "companydocument"]:
                                title = r[1]
                                url = r[2] if len(r) > 2 else ""
                                content = ""
                                if "drive.google.com" in url:
                                    # B√°o c√°o ƒëang ƒë·ªçc file n√†o
                                    SYNC_STATE["detail"] = f"ƒêang ƒë·ªçc file: {title[:15]}..."
                                    print(f"   üì• ƒê·ªçc file: {title[:20]}...")
                                    content = get_text_from_drive(url)
                                final_text = f"{title}. CHI TI·∫æT: {content}. Link: {url}"
                            else:
                                final_text = r[1]
                            
                            batch_texts.append(final_text)
                            batch_ids.append(rid)

                        # T·∫°o Embedding LOCAL
                        vectors = get_embeddings_batch(batch_texts)
                        
                        # L∆∞u v√†o DB
                        if vectors:
                            for idx, vec in enumerate(vectors):
                                cur.execute(f'UPDATE "{table}" SET embedding = %s, last_content_indexed = %s WHERE "{id_col}" = %s', 
                                            (vec, batch_texts[idx], batch_ids[idx]))
                            conn.commit()
                            print(f"   ‚úÖ Saved batch {i//BATCH_SIZE + 1}.")

        print(f"üéâ [Sync] Ho√†n t·∫•t sau {time.time() - t_start:.2f}s.")
        SYNC_STATE["step"] = "Ho√†n t·∫•t"
        SYNC_STATE["detail"] = f"T·ªïng th·ªùi gian: {time.time() - t_start:.2f}s"
        
    except Exception as e:
        print(f"‚ùå L·ªói Sync: {e}")
        SYNC_STATE["step"] = "L·ªói"
        SYNC_STATE["detail"] = str(e)
    finally:
        # ƒê·ª£i 5s r·ªìi t·∫Øt tr·∫°ng th√°i running ƒë·ªÉ FE k·ªãp ƒë·ªçc th√¥ng b√°o "Ho√†n t·∫•t"
        time.sleep(5) 
        SYNC_STATE["is_running"] = False
# ==================== 7. CV REVIEW (MATCH MAKER) ====================

# ==================== 7. CV REVIEW (PHI√äN B·∫¢N CHUY√äN GIA CAO C·∫§P) ====================

def run_cv_review(cv_text: str, user_message: str):
    from rag_core import start_chat_session, get_chat_response
    
    # 1. Ki·ªÉm tra ƒë·∫ßu v√†o
    print(f"üìÑ [CV Review] ƒêang ƒë·ªçc CV: {len(cv_text)} k√Ω t·ª±.")
    if len(cv_text) < 100:
        return "‚ö†Ô∏è L·ªói: Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung CV (File ·∫£nh ho·∫∑c l·ªói font).", "CV Error"

    # 2. T√¨m ki·∫øm Job ph√π h·ª£p trong DB
    # Th√™m t·ª´ kh√≥a "JD" v√† "M√¥ t·∫£ c√¥ng vi·ªác" ƒë·ªÉ t√¨m ƒë√∫ng file tuy·ªÉn d·ª•ng
    search_query = cv_text[:500] + " tuy·ªÉn d·ª•ng JD Job Description y√™u c·∫ßu k·ªπ nƒÉng l·∫≠p tr√¨nh"
    db_context = search_vectors(search_query)
    
    # 3. --- T√çNH NƒÇNG M·ªöI: ƒê·ªåC CHI TI·∫æT FILE JD (Real-time) ---
    # N·∫øu Vector Search t√¨m th·∫•y link Drive c·ªßa JD, ta s·∫Ω t·∫£i v·ªÅ ƒë·ªçc ngay l·∫≠p t·ª©c
    detailed_jds = ""
    found_links = re.findall(r'https://drive\.google\.com/[^\s]+', db_context)
    
    # Ch·ªâ ƒë·ªçc t·ªëi ƒëa 2 file JD li√™n quan nh·∫•t ƒë·ªÉ kh√¥ng b·ªã qu√° t·∫£i
    if found_links:
        print(f"üöÄ [CV Match] Ph√°t hi·ªán {len(found_links)} JD ti·ªÅm nƒÉng. ƒêang ƒë·ªçc chi ti·∫øt...")
        unique_links = list(set(found_links))[:2] # L·∫•y 2 link ƒë·∫ßu ti√™n (th∆∞·ªùng l√† match nh·∫•t)
        
        for idx, link in enumerate(unique_links):
            link = link.rstrip(").,")
            content = get_text_from_drive(link) # H√†m n√†y ƒë√£ c√≥ s·∫µn trong agent_adk.py
            if content:
                detailed_jds += f"\n--- CHI TI·∫æT JD S·ªê {idx+1} ({link}) ---\n{content[:4000]}\n" # C·∫Øt b·ªõt n·∫øu qu√° d√†i
                print(f"   ‚úÖ ƒê√£ ƒë·ªçc xong JD s·ªë {idx+1}")
    else:
        detailed_jds = "Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file chi ti·∫øt (Ch·ªâ d√πng t√≥m t·∫Øt h·ªá th·ªëng)."

    # 4. Prompt Chuy√™n Gia Tuy·ªÉn D·ª•ng (Si√™u chi ti·∫øt)
    prompt = f"""
    VAI TR√í: B·∫°n l√† Chuy√™n gia Tuy·ªÉn d·ª•ng (Senior Tech Recruiter) v·ªõi 20 nƒÉm kinh nghi·ªám.
    
    D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO:
    --------------------------------------------------
    1. H·ªí S∆† ·ª®NG VI√äN (CV):
    {cv_text[:3000]}
    
    2. DANH S√ÅCH VI·ªÜC L√ÄM T√åM TH·∫§Y (T√≥m t·∫Øt):
    {db_context}
    
    3. N·ªòI DUNG JD ƒê·∫¶Y ƒê·ª¶ (QUAN TR·ªåNG - ∆Øu ti√™n d√πng th√¥ng tin n√†y):
    {detailed_jds}
    
    4. Y√äU C·∫¶U C·ª¶A NG∆Ø·ªúI D√ôNG: "{user_message}"
    --------------------------------------------------
    
    NHI·ªÜM V·ª§:
    H√£y ƒë√≥ng vai ng∆∞·ªùi Mentor, ph√¢n t√≠ch k·ªπ l∆∞·ª°ng s·ª± ph√π h·ª£p gi·ªØa CV v√† c√°c v·ªã tr√≠ t√¨m ƒë∆∞·ª£c.
    Tuy·ªát ƒë·ªëi KH√îNG tr·∫£ l·ªùi chung chung. Ph·∫£i ƒë∆∞a ra d·∫´n ch·ª©ng c·ª• th·ªÉ t·ª´ CV v√† JD.
    
    ƒê·ªäNH D·∫†NG C√ÇU TR·∫¢ L·ªúI (B·∫Øt bu·ªôc tu√¢n th·ªß):
    
    üåü **ƒê·ªÄ XU·∫§T S·ªê 1: [T√™n V·ªã Tr√≠] - [T√™n C√¥ng Ty]**
       * **üéØ ƒê·ªô ph√π h·ª£p:** [ƒêi·ªÉm s·ªë/10] (D·ª±a tr√™n k·ªπ nƒÉng kh·ªõp)
       * **‚úÖ T·∫°i sao b·∫°n ph√π h·ª£p (Matching Points):**
           - CV b·∫°n c√≥ k·ªπ nƒÉng [A] kh·ªõp v·ªõi y√™u c·∫ßu [B] trong JD.
           - B·∫°n ƒë√£ l√†m ƒë·ªì √°n [C] li√™n quan ƒë·∫øn m·∫£ng [D] c·ªßa c√¥ng ty.
       * **‚ö†Ô∏è ƒêi·ªÉm c√≤n thi·∫øu (Gap Analysis):**
           - C√¥ng ty y√™u c·∫ßu [X] (c√≥ trong JD) nh∆∞ng CV b·∫°n ch∆∞a th·∫•y nh·∫Øc ƒë·∫øn.
           - C·∫ßn c·∫£i thi·ªán th√™m v·ªÅ [K·ªπ nƒÉng m·ªÅm/Ti·∫øng Anh] theo y√™u c·∫ßu c·ªßa h·ªç.
       * **üéÅ Quy·ªÅn l·ª£i n·ªïi b·∫≠t (N·∫øu c√≥ trong JD):** [L∆∞∆°ng/Tr·ª£ c·∫•p/M√¥i tr∆∞·ªùng...]
       * **üîó Link t√†i li·ªáu:** [Link file Drive ho·∫∑c Email n·∫øu c√≥]

    üåü **ƒê·ªÄ XU·∫§T S·ªê 2: ...** (T∆∞∆°ng t·ª±)

    üí° **L·ªúI KHUY√äN T·ªîNG QU√ÅT:**
    (ƒê∆∞a ra 1 l·ªùi khuy√™n ƒë·∫Øt gi√° ƒë·ªÉ ·ª©ng vi√™n c·∫£i thi·ªán CV n√†y t·ªët h∆°n).
    """
    
    print("ü§ñ [CV Review] ƒêang ph√¢n t√≠ch s√¢u...")
    # G·ªçi AI tr·∫£ l·ªùi
    answer = get_chat_response(start_chat_session(), prompt)
    
    return answer, "CV Matcher"
