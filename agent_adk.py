import os
import psycopg2
import vertexai
from vertexai.language_models import TextEmbeddingModel

# ==================== C·∫§U H√åNH & INIT ====================
key_path = "rag-service-account.json"
if os.path.exists(key_path):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath(key_path)

PROJECT_ID = "reflecting-surf-477600-p4"
LOCATION = "europe-west4"
DB_DSN = "postgresql://postgres:123@caboose.proxy.rlwy.net:54173/railway"

embedding_model = None
try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    print("‚úÖ [Agent] Vertex AI Ready.")
except Exception as e:
    print(f"‚ö†Ô∏è [Agent] Init Error: {e}")

# ==================== CORE FUNCTIONS ====================

def get_query_embedding(text):
    if not embedding_model: return None
    try:
        return embedding_model.get_embeddings([text[:2000]])[0].values
    except: return None

def search_vectors(question, target_table="auto", limit=5):
    """
    T√¨m ki·∫øm th√¥ng minh: T·ª± ƒë·ªông ch·ªçn b·∫£ng v√† x·ª≠ l√Ω l·ªói NULL an to√†n
    """
    print(f"üîç [Search] ƒêang t√¨m: '{question}'...")
    query_vector = get_query_embedding(question)
    if not query_vector: return "L·ªói h·ªá th·ªëng: Kh√¥ng t·∫°o ƒë∆∞·ª£c vector."

    conn = None
    try:
        conn = psycopg2.connect(dsn=DB_DSN)
        cur = conn.cursor()
        
        # 1. LOGIC CH·ªåN B·∫¢NG TH√îNG MINH
        tables_to_search = []
        q_lower = question.lower()
        
        # T·ª± ƒë·ªông ph√°t hi·ªán √Ω ƒë·ªãnh
        if any(k in q_lower for k in ["ojt", "t√†i li·ªáu", "quy tr√¨nh", "h∆∞·ªõng d·∫´n", "gi·ªõi thi·ªáu", "h·ªçc k·ª≥"]):
            tables_to_search.append("ojtdocument")
        
        if any(k in q_lower for k in ["job", "vi·ªác", "l∆∞∆°ng", "tuy·ªÉn", "v·ªã tr√≠", "dev", "java", "net", "th·ª±c t·∫≠p"]):
            tables_to_search.append("job_position")
            
        # M·∫∑c ƒë·ªãnh t√¨m c·∫£ 2 n·∫øu kh√¥ng r√µ
        if not tables_to_search:
            tables_to_search = ["ojtdocument", "job_position"]

        # N·∫øu AI ch·ªâ ƒë·ªãnh r√µ (override)
        if target_table == "ojtdocument": tables_to_search = ["ojtdocument"]
        elif target_table == "job_position": tables_to_search = ["job_position"]

        final_results = []
        
        # 2. CH·∫†Y T√åM KI·∫æM TR√äN T·ª™NG B·∫¢NG
        for table in tables_to_search:
            if table == "ojtdocument":
                cols = "title, file_url"
                prefix = "T√ÄI LI·ªÜU"
            elif table == "job_position":
                cols = "job_title, requirements, location, salary"
                prefix = "C√îNG VI·ªÜC"
            else:
                continue

            # SQL: Th√™m ƒëi·ªÅu ki·ªán embedding IS NOT NULL ƒë·ªÉ tr√°nh l·ªói
            sql = f"""
                SELECT {cols}, 1 - (embedding <=> %s::vector) as similarity
                FROM "{table}"
                WHERE embedding IS NOT NULL 
                ORDER BY embedding <=> %s::vector
                LIMIT 3;
            """
            cur.execute(sql, (query_vector, query_vector))
            rows = cur.fetchall()
            
            for row in rows:
                # --- S·ª¨A L·ªñI ·ªû ƒê√ÇY: Ki·ªÉm tra None tr∆∞·ªõc khi d√πng ---
                similarity = row[-1]
                
                if similarity is None: 
                    continue # B·ªè qua d√≤ng l·ªói

                if similarity > 0.40: # ƒê·ªô kh·ªõp > 40%
                    content = ", ".join([str(item) for item in row[:-1] if item is not None])
                    final_results.append(f"[{prefix}] {content} (ƒê·ªô kh·ªõp: {similarity:.2f})")

        if not final_results:
            return "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ph√π h·ª£p trong h·ªá th·ªëng."
            
        return "\n".join(final_results)

    except Exception as e:
        print(f"‚ùå DB Error: {e}")
        return f"L·ªói Database: {e}"
    finally:
        if conn: conn.close()

# ==================== LOGIC CHAT ====================

def run_agent(question: str, file_content: str = None):
    from rag_core import start_chat_session, get_chat_response
    
    prompt = question
    if file_content:
        prompt = f"Th√¥ng tin b·ªï sung:\n{file_content}\n\nC√¢u h·ªèi: {question}"

    chat_session = start_chat_session()
    response = get_chat_response(chat_session, prompt)
    return response, "Mode: Vector Search"

def run_cv_review(cv_text: str, user_message: str):
    from rag_core import start_chat_session
    
    matched_jobs = search_vectors(cv_text, target_table="job_position", limit=3)
    
    prompt = f"""
    B·∫°n l√† chuy√™n gia tuy·ªÉn d·ª•ng. 
    CV ·ª®ng vi√™n: {cv_text[:3000]}
    Job ph√π h·ª£p: {matched_jobs}
    C√¢u h·ªèi: "{user_message}"
    
    H√£y ƒë∆∞a ra l·ªùi khuy√™n v√† g·ª£i √Ω job ph√π h·ª£p.
    """
    
    chat_session = start_chat_session()
    response = chat_session.send_message(prompt)
    return response.text, "Mode: CV Review"