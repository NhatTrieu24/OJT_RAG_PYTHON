import os
import time
import psycopg2
import vertexai
from vertexai.language_models import TextEmbeddingModel

# ==================== 1. C·∫§U H√åNH AUTHENTICATION ====================
key_path = "rag-service-account.json"
if os.path.exists(key_path):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath(key_path)

PROJECT_ID = os.getenv("PROJECT_ID", "reflecting-surf-477600-p4")
LOCATION = os.getenv("LOCATION", "europe-west4")
DB_DSN = os.getenv("DB_DSN")

embedding_model = None
try:
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    print("‚úÖ [Agent] Vertex AI & Embedding Model Ready.")
except Exception as e:
    print(f"‚ö†Ô∏è [Agent] Init Error: {e}")

# ==================== 2. H√ÄM ƒê·ªíNG B·ªò VECTOR (UPGRADED) ====================

def sync_missing_embeddings():
    """
    ƒê·ªìng b·ªô Vector cho t·∫•t c·∫£ c√°c b·∫£ng nghi·ªáp v·ª•. 
    H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông qu√©t c√°c d√≤ng c√≥ embedding = NULL ƒë·ªÉ x·ª≠ l√Ω.
    """
    print("üîÑ [System] B·∫Øt ƒë·∫ßu qu√©t d·ªØ li·ªáu ƒë·ªÉ ƒë·ªìng b·ªô Vector...")
    conn = None
    try:
        conn = psycopg2.connect(dsn=DB_DSN)
        cur = conn.cursor()
        
        # Danh s√°ch c√°c b·∫£ng c·∫ßn Vector h√≥a d·ªØ li·ªáu
        targets = [
            ("semester", "name", "semester_id"),
            ("major", "major_title", "major_id"),
            ("company", "name", "company_id"),
            ("ojtdocument", "title", "ojtdocument_id"),
            ("job_position", "job_title", "job_position_id"),
            ("job_description", "job_description", "job_description_id"),
            ("finalreport", "student_report_text", "final_report_id"),
            ("companydocument", "title", "company_document_id")
        ]
        
        updated_total = 0
        for table, text_col, id_col in targets:
            existing_cols = get_existing_columns(cur, table)
            
            if text_col in existing_cols and "embedding" in existing_cols:
                cur.execute(f"SELECT {id_col}, {text_col} FROM \"{table}\" WHERE embedding IS NULL")
                rows = cur.fetchall()
                
                if not rows: continue
                
                print(f"   ‚àü B·∫£ng [{table}]: T√¨m th·∫•y {len(rows)} d√≤ng c·∫ßn x·ª≠ l√Ω...")
                for row_id, text in rows:
                    if not text or len(str(text).strip()) < 2: continue
                    
                    vector = get_query_embedding(str(text))
                    if vector:
                        cur.execute(f"UPDATE \"{table}\" SET embedding = %s WHERE {id_col} = %s", (vector, row_id))
                        updated_total += 1
                        
                        # Ngh·ªâ ƒë·ªÉ tr√°nh l·ªói Quota 429 c·ªßa Google Cloud
                        time.sleep(0.5) 
                        
                        if updated_total % 10 == 0:
                            conn.commit()
                            print(f"      - ƒê√£ xong {updated_total} d√≤ng...")
            
        conn.commit()
        print(f"‚úÖ [System] Ho√†n t·∫•t! T·ªïng c·ªông c·∫≠p nh·∫≠t: {updated_total} Vector.")
            
    except Exception as e:
        print(f"‚ùå [System] L·ªói ƒë·ªìng b·ªô: {e}")
    finally:
        if conn: conn.close()

# ==================== 3. H√ÄM CORE: T·∫†O VECTOR & SEARCH ====================

def get_query_embedding(text):
    """Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh Vector 768 chi·ªÅu"""
    if not embedding_model or not text: return None
    try:
        # C·∫Øt ng·∫Øn text ƒë·ªÉ tr√°nh l·ªói Token Limit (Embedding model th∆∞·ªùng gi·ªõi h·∫°n ~2048 tokens)
        clean_text = str(text).replace("\n", " ")[:3000]
        embeddings = embedding_model.get_embeddings([clean_text])
        return embeddings[0].values
    except Exception as e:
        print(f"‚ùå Embedding Error: {e}")
        return None

def get_existing_columns(cur, table_name):
    """L·∫•y danh s√°ch c·ªôt th·ª±c t·∫ø c·ªßa b·∫£ng ƒë·ªÉ tr√°nh l·ªói SQL khi c·∫•u h√¨nh thay ƒë·ªïi"""
    try:
        cur.execute(f"SELECT * FROM \"{table_name}\" LIMIT 0")
        return [desc[0] for desc in cur.description]
    except:
        return []

def search_vectors(question, target_table="auto", limit=5):
    """
    T√¨m ki·∫øm ng·ªØ nghƒ©a (Semantic Search) s·ª≠ d·ª•ng Cosine Similarity.
    """
    print(f"üîç [Search] Ph√¢n t√≠ch c√¢u h·ªèi: '{question}'...")
    query_vector = get_query_embedding(question)
    if not query_vector: return "Kh√¥ng th·ªÉ t·∫°o vector t√¨m ki·∫øm."

    conn = None
    try:
        conn = psycopg2.connect(dsn=DB_DSN)
        cur = conn.cursor()
        
        # 1. Ph√¢n lo·∫°i √Ω ƒë·ªãnh th√¥ng minh
        tables_to_search = []
        q_lower = question.lower()
        
        mapping = {
            "ojtdocument": ["ojt", "quy ƒë·ªãnh", "h∆∞·ªõng d·∫´n", "quy tr√¨nh", "bi·ªÉu m·∫´u", "h·ª£p ƒë·ªìng", "t√†i li·ªáu"],
            "job_position": ["vi·ªác l√†m", "tuy·ªÉn d·ª•ng", "job", "l∆∞∆°ng", "salary", "v·ªã tr√≠", "th·ª±c t·∫≠p", "dev", "engineer"],
            "company": ["c√¥ng ty", "ƒë·ªãa ch·ªâ", "website", "li√™n h·ªá", "m√£ s·ªë thu·∫ø", "tax"],
            "semester": ["k·ª≥ h·ªçc", "semester", "spring", "summer", "fall", "th·ªùi gian", "b·∫Øt ƒë·∫ßu"],
            "major": ["ng√†nh", "chuy√™n ng√†nh", "major", "kh·ªëi ng√†nh"]
        }

        if target_table in mapping.keys():
            tables_to_search = [target_table]
        else:
            for table, keywords in mapping.items():
                if any(k in q_lower for k in keywords):
                    tables_to_search.append(table)
        
        if not tables_to_search:
            tables_to_search = ["ojtdocument", "job_position"]

        final_results = []
        
        # 2. Truy v·∫•n d·ªØ li·ªáu
        for table in tables_to_search:
            cols = get_existing_columns(cur, table)
            if "embedding" not in cols: continue

            # ƒê·ªãnh nghƒ©a c√°c c·ªôt quan tr·ªçng mu·ªën l·∫•y d·ªØ li·ªáu tr·∫£ v·ªÅ cho AI
            display_map = {
                "ojtdocument": ["title", "file_url"],
                "job_position": ["job_title", "salary_range", "location", "requirements"],
                "company": ["name", "address", "website"],
                "semester": ["name", "start_date", "end_date"],
                "major": ["major_title", "major_code"]
            }
            
            selected_cols = [c for c in display_map.get(table, cols) if c in cols]
            if not selected_cols: selected_cols = cols[:3]
            
            cols_sql = ", ".join([f"\"{c}\"" for c in selected_cols])
            
            # S·ª≠ d·ª•ng to√°n t·ª≠ <=> (Cosine Distance) c·ªßa pgvector
            sql = f"""
                SELECT {cols_sql}, 1 - (embedding <=> %s::vector) as similarity
                FROM "{table}"
                WHERE embedding IS NOT NULL 
                ORDER BY embedding <=> %s::vector
                LIMIT %s;
            """
            cur.execute(sql, (query_vector, query_vector, limit))
            rows = cur.fetchall()
            
            for row in rows:
                score = row[-1]
                if score and score > 0.38: # Ng∆∞·ª°ng ch√≠nh x√°c
                    info = " | ".join([f"{selected_cols[i]}: {row[i]}" for i in range(len(selected_cols)) if row[i]])
                    final_results.append(f"[{table.upper()}] {info} (Kh·ªõp: {score:.2f})")

        return "\n".join(final_results) if final_results else "KH√îNG T√åM TH·∫§Y D·ªÆ LI·ªÜU PH√ô H·ª¢P."

    except Exception as e:
        return f"L·ªói DB: {str(e)}"
    finally:
        if conn: conn.close()

# ==================== 4. LOGIC ƒêI·ªÄU PH·ªêI (ORCHESTRATION) ====================

def run_agent(question: str, file_content: str = None):
    """
    Lu·ªìng x·ª≠ l√Ω RAG: Search DB -> T·∫°o Context -> AI tr·∫£ l·ªùi
    """
    from rag_core import start_chat_session, get_chat_response
    
    # T√¨m ki·∫øm context t·ª´ Database
    db_context = search_vectors(question)
    
    # X√¢y d·ª±ng Prompt "Si√™u ng·ªØ c·∫£nh"
    prompt = f"""
    D∆∞·ªõi ƒë√¢y l√† D·ªÆ LI·ªÜU TH·ª∞C T·∫æ t·ª´ h·ªá th·ªëng:
    {db_context}
    ---
    D·ªØ li·ªáu t·ª´ file ng∆∞·ªùi d√πng cung c·∫•p: {file_content if file_content else "N/A"}
    ---
    C√ÇU H·ªéI: {question}
    ---
    Y√äU C·∫¶U: D·ª±a v√†o D·ªÆ LI·ªÜU TH·ª∞C T·∫æ ·ªü tr√™n ƒë·ªÉ tr·∫£ l·ªùi. N·∫øu kh√¥ng th·∫•y th√¥ng tin trong d·ªØ li·ªáu, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong h·ªá th·ªëng".
    """
    
    chat_session = start_chat_session()
    return get_chat_response(chat_session, prompt), "Mode: RAG Vector Search"

def run_cv_review(cv_text: str, user_message: str):
    """X·ª≠ l√Ω Review CV d·ª±a tr√™n c√°c Job th·ª±c t·∫ø ƒëang c√≥"""
    from rag_core import start_chat_session
    
    matched_jobs = search_vectors(cv_text, target_table="job_position", limit=3)
    
    prompt = f"""
    B·∫°n l√† HR chuy√™n nghi·ªáp. H√£y ph√¢n t√≠ch CV n√†y: {cv_text[:3000]}
    D·ª±a tr√™n c√°c v·ªã tr√≠ th·ª±c t·∫ø sau: {matched_jobs}
    H√£y t∆∞ v·∫•n cho ·ª©ng vi√™n theo y√™u c·∫ßu: {user_message}
    """
    
    chat_session = start_chat_session()
    response = chat_session.send_message(prompt)
    return response.text, "Mode: CV Reviewer"
